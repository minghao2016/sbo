<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Text prediction via N-gram Stupid Back-off models • sbo</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Text prediction via N-gram Stupid Back-off models">
<meta property="og:description" content="sbo">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">sbo</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/sbo.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="http://github.com/vgherard/sbo/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="sbo_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Text prediction via N-gram Stupid Back-off models</h1>
                        <h4 class="author">Valerio Gherardi</h4>
                  <a class="author_email" href="mailto:#"></a><a href="mailto:vgherard@sissa.it" class="email">vgherard@sissa.it</a>
      
                  
            <h4 class="date">2020-08-13</h4>
      
      <small class="dont-index">Source: <a href="http://github.com/vgherard/sbo/blob/master/vignettes/sbo.Rmd"><code>vignettes/sbo.Rmd</code></a></small>
      <div class="hidden name"><code>sbo.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>The <code>sbo</code> package provides utilities for building and evaluating next-word prediction functions based on <a href="https://www.aclweb.org/anthology/D07-1090.pdf">Stupid Back-off</a> <a href="https://en.wikipedia.org/wiki/N-gram">N-gram models</a> in R. In this vignette, I illustrate the functions and classes exported by <code>sbo</code>, the typical workflow for building a text predictor from a given training corpus, and the evaluation of next-word predictions through a test corpus. In the last section, I list some upcoming features in a future version of <code>sbo</code>.</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="no">sbo</span>)</pre></body></html></div>
</div>
<div id="functions-and-classes" class="section level2">
<h2 class="hasAnchor">
<a href="#functions-and-classes" class="anchor"></a>Functions and classes</h2>
<p>The <code>sbo</code> package pivots around two (S3) object classes:</p>
<ul>
<li>
<code>kgram_freqs</code>: A collection of <span class="math inline">\(k\)</span>-gram frequency tables, with <span class="math inline">\(k\)</span> up to a given order <span class="math inline">\(N\)</span>.</li>
<li>
<code>sbo_preds</code>: A collection of tables employed to store and retrieve next-word predictions in a compact and efficient way.</li>
</ul>
<p>The functions <code>get_word_freqs</code> and <code>get_kgram_freqs</code> are used to extract word and <span class="math inline">\(k\)</span>-gram frequency tables from a training corpus, and the function <code>build_sbo_preds</code> constructs a next-word prediction table from a <code>kgram_freqs</code> object. I illustrate the entire process of building a text-prediction function from a training corpus in the next section.</p>
</div>
<div id="building-a-next-word-prediction-function-with-sbo" class="section level2">
<h2 class="hasAnchor">
<a href="#building-a-next-word-prediction-function-with-sbo" class="anchor"></a>Building a next-word prediction function with <code>sbo</code>
</h2>
<p>In this and the next section we will employ the <code>twitter_train</code> and <code>twitter_test</code> example datasets, included in <code>sbo</code> for illustrative purpose:</p>
<div class="sourceCode" id="cb2"><html><body><pre class="r"><span class="no">train</span> <span class="kw">&lt;-</span> <span class="no">twitter_train</span>
<span class="no">test</span> <span class="kw">&lt;-</span> <span class="no">twitter_test</span></pre></body></html></div>
<p>These are small samples of <span class="math inline">\(10^5\)</span> and <span class="math inline">\(10^4\)</span> entries, respectively, from the “Tweets” Swiftkey dataset fully available <a href="https://www.kaggle.com/crmercado/tweets-blogs-news-swiftkey-dataset-4million">here</a>. Each entry consists of a single tweet in English, <em>e.g.</em>:</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span>(<span class="no">train</span>, <span class="fl">3</span>)
<span class="co">#&gt; [1] "Stooges are doing some nice non-traditional"                       </span>
<span class="co">#&gt; [2] "You are so loved Sheridan !"                                       </span>
<span class="co">#&gt; [3] "Yup so the next fight WILL have to be pacman vs. Mayweather. Srsly"</span></pre></body></html></div>
<p>The prototypical workflow for building a text-predictor in <code>sbo</code> goes as follows:</p>
<p><em>Step 0 (optional)</em>. Build a dictionary from training set, typically keeping the top <span class="math inline">\(V\)</span> most frequent words:</p>
<div class="sourceCode" id="cb4"><html><body><pre class="r"><span class="co"># N.B.: get_word_freqs(train) returns a tibble with a 'word' column </span>
<span class="co"># and a 'counts' column, sorted by decreasing counts.</span>
<span class="no">dict</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/get_word_freqs.html">get_word_freqs</a></span>(<span class="no">train</span>) <span class="kw">%$%</span> <span class="no">word</span>[<span class="fl">1</span>:<span class="fl">1000</span>]
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span>(<span class="no">dict</span>)
<span class="co">#&gt; [1] "the" "to"  "i"   "a"   "you" "and"</span></pre></body></html></div>
<p>Alternatively, one may use a predefined dictionary.</p>
<p><em>Step 1</em>. Get <span class="math inline">\(k\)</span>-gram frequencies from training corpus:</p>
<div class="sourceCode" id="cb5"><html><body><pre class="r">( <span class="no">freqs</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/get_kgram_freqs.html">get_kgram_freqs</a></span>(<span class="no">train</span>, <span class="no">dict</span>, <span class="kw">N</span> <span class="kw">=</span> <span class="fl">3</span>) ) <span class="co"># 'N' is the order of n-grams</span>
<span class="co">#&gt; k-gram frequency table </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Order (N): 3 </span>
<span class="co">#&gt; Dictionary size: 1000  words</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; # of unique 1-grams: 1002 </span>
<span class="co">#&gt; # of unique 2-grams: 102627 </span>
<span class="co">#&gt; # of unique 3-grams: 410732 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Object size: 13 Mb </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; See ?get_kgram_freqs for help.</span></pre></body></html></div>
<p><em>Step 2</em>. Build next-word prediction tables:</p>
<div class="sourceCode" id="cb6"><html><body><pre class="r">( <span class="no">sbo</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/build_sbo_preds.html">build_sbo_preds</a></span>(<span class="no">freqs</span>) )
<span class="co">#&gt; Next-word prediction table for Stupid Backoff n-gram model </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Order (N): 3 </span>
<span class="co">#&gt; Dictionary size: 1000  words</span>
<span class="co">#&gt; Backoff penalization (lambda): 0.4 </span>
<span class="co">#&gt; Maximum number of predictions (L): 3 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Object size: 1.8 Mb </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; See ?build_sbo_preds, ?predict.sbo_preds for help.</span></pre></body></html></div>
<p>At this point we can predict next words from our model, by using <code>predict</code> (see <code><a href="../reference/predict.sbo_preds.html">?predict.sbo_preds</a></code> for help on the relevant <code>predict</code> method):</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">sbo</span>, <span class="st">"i love"</span>) <span class="co"># a character vector</span>
<span class="co">#&gt; [1] "you" "it"  "my"</span>
<span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span>(<span class="no">sbo</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"Colorless green ideas sleep"</span>, <span class="st">"See you"</span>)) <span class="co"># a char matrix</span>
<span class="co">#&gt;                             [,1]    [,2] [,3] </span>
<span class="co">#&gt; Colorless green ideas sleep "."     "in" "and"</span>
<span class="co">#&gt; See you                     "there" "."  "at"</span></pre></body></html></div>
<p>Last, but not least, we can employ our model for generating some beautiful non-sense:</p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="fl">840</span>)
<span class="fu"><a href="../reference/babble.html">babble</a></span>(<span class="no">sbo</span>)
<span class="co">#&gt; [1] "lots more of a good day to the next one of the best of the year of the best."</span>
<span class="fu"><a href="../reference/babble.html">babble</a></span>(<span class="no">sbo</span>)
<span class="co">#&gt; [1] "they don't like."</span>
<span class="fu"><a href="../reference/babble.html">babble</a></span>(<span class="no">sbo</span>)
<span class="co">#&gt; [1] "remember to be."</span></pre></body></html></div>
<p>If we wish to save the frequency tables, or the final prediction tables, and reload them in a future session, we can easily do this through <code>save</code>/<code>load</code>, <em>e.g.</em></p>
<div class="sourceCode" id="cb9"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/save.html">save</a></span>(<span class="no">sbo</span>, <span class="st">"sbo.rda"</span>)
<span class="fu"><a href="https://rdrr.io/r/base/load.html">load</a></span>(<span class="st">"sbo.rda"</span>)</pre></body></html></div>
<p>For convenience, the objects created in this section are preloaded in <code>sbo</code> as <code>twitter_dict</code>, <code>twitter_freqs</code> and <code>twitter_sbo</code>.</p>
<div id="some-details-on-text-preprocessing-and-k-gram-tokenization" class="section level3">
<h3 class="hasAnchor">
<a href="#some-details-on-text-preprocessing-and-k-gram-tokenization" class="anchor"></a>Some details on text preprocessing and <span class="math inline">\(k\)</span>-gram tokenization</h3>
<p>At the present stage, both <code>get_word_freqs</code> and <code>get_kgram_freqs</code> employ internal functions for text preprocessing and tokenization. Preprocessing consists of the following steps, in this order:</p>
<ol style="list-style-type: decimal">
<li>Lower-case everything.</li>
<li>Replace all punctation including <strong>.</strong>, <strong>?</strong>, <strong>!</strong>, <strong>:</strong>, <strong>;</strong> (any number of any of these) with a single <strong>.</strong>.</li>
<li>Strip any character different from <strong>.</strong>, <strong>’</strong>, <strong>space</strong> or alphanumeric.</li>
<li>Split sentences in correspondence of dots and wrap each sentence with appropriate Begin/End-Of-Sentence tokens.</li>
</ol>
<p>Words (including the Begin/End-Of-Sentence tokens) are thus tokenized by splitting sentences in correspondence of <strong>space</strong>. In <code>get_kgram_freqs</code>, each out-of-vocabulary word is replaced by an unknown word token.</p>
</div>
</div>
<div id="evaluating-next-word-predictions" class="section level2">
<h2 class="hasAnchor">
<a href="#evaluating-next-word-predictions" class="anchor"></a>Evaluating next-word predictions</h2>
<p>Once we have built our next-word predictor, we may want to directly test its predictions on an independent corpus. For this purpose, <code>sbo</code> offers the function <code>eval_sbo_preds</code>, which performs the following test:</p>
<ol style="list-style-type: decimal">
<li>Sample a single <span class="math inline">\(N\)</span>-gram from each sentence of test corpus.</li>
<li>Predict next words from the <span class="math inline">\((N-1)\)</span>-gram prefix.</li>
<li>Return all predictions, together with the true word completions.</li>
</ol>
<p>As a concrete example, we test the text-predictor trained in the previous section over the Twitter (independent) test set.</p>
<div class="sourceCode" id="cb10"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="fl">840</span>)
( <span class="no">eval</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/eval_sbo_preds.html">eval_sbo_preds</a></span>(<span class="no">sbo</span>, <span class="no">test</span>) )
<span class="co">#&gt; # A tibble: 18,315 x 4</span>
<span class="co">#&gt;    input            true      preds        correct</span>
<span class="co">#&gt;    &lt;chr&gt;            &lt;chr&gt;     &lt;named list&gt; &lt;lgl&gt;  </span>
<span class="co">#&gt;  1 "oh hey"         shirtless &lt;chr [3]&gt;    FALSE  </span>
<span class="co">#&gt;  2 " "              how       &lt;chr [3]&gt;    FALSE  </span>
<span class="co">#&gt;  3 " ah"            no        &lt;chr [3]&gt;    FALSE  </span>
<span class="co">#&gt;  4 "he estudiado"   .         &lt;chr [3]&gt;    TRUE   </span>
<span class="co">#&gt;  5 "nada d"         .         &lt;chr [3]&gt;    TRUE   </span>
<span class="co">#&gt;  6 "mama no"        esta      &lt;chr [3]&gt;    FALSE  </span>
<span class="co">#&gt;  7 "ya mean"        .         &lt;chr [3]&gt;    TRUE   </span>
<span class="co">#&gt;  8 "tennis the"     scoring   &lt;chr [3]&gt;    FALSE  </span>
<span class="co">#&gt;  9 " thanks"        for       &lt;chr [3]&gt;    TRUE   </span>
<span class="co">#&gt; 10 "concert wasn't" over      &lt;chr [3]&gt;    TRUE   </span>
<span class="co">#&gt; # … with 18,305 more rows</span></pre></body></html></div>
<p>As it is seen, <code>eval_sbo_preds</code> returns a tibble containing the input <span class="math inline">\((N-1)\)</span>-grams, the true completions, the predicted completions and a column indicating whether one of the predictions were correct or not.</p>
<p>We can estimate predictive accuracy as follows (the uncertainty in the estimate is approximated by the binomial formula <span class="math inline">\(\sigma = \sqrt{\frac{p(1-p)}{M}}\)</span>, where <span class="math inline">\(M\)</span> is the number of trials):</p>
<div class="sourceCode" id="cb11"><html><body><pre class="r"><span class="no">eval</span> <span class="kw">%&gt;%</span> <span class="fu">summarise</span>(<span class="kw">accuracy</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span>(<span class="no">correct</span>)/<span class="fu">n</span>(),
                   <span class="kw">uncertainty</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>( <span class="no">accuracy</span>*(<span class="fl">1</span>-<span class="no">accuracy</span>) / <span class="fu">n</span>() )
                   )
<span class="co">#&gt; # A tibble: 1 x 2</span>
<span class="co">#&gt;   accuracy uncertainty</span>
<span class="co">#&gt;      &lt;dbl&gt;       &lt;dbl&gt;</span>
<span class="co">#&gt; 1    0.353     0.00353</span></pre></body></html></div>
<p>We may want to exclude from the test <span class="math inline">\(N\)</span>-grams ending by the End-Of-Sentence token (here represented by <code>"."</code>):</p>
<div class="sourceCode" id="cb12"><html><body><pre class="r"><span class="no">eval</span> <span class="kw">%&gt;%</span> <span class="co"># Accuracy for in-sentence predictions</span>
        <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span>(<span class="no">true</span> <span class="kw">!=</span> <span class="st">"."</span>) <span class="kw">%&gt;%</span>
        <span class="fu">summarise</span>(<span class="kw">accuracy</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span>(<span class="no">correct</span>)/<span class="fu">n</span>(),
                  <span class="kw">uncertainty</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>( <span class="no">accuracy</span>*(<span class="fl">1</span>-<span class="no">accuracy</span>) / <span class="fu">n</span>() )
                  )
<span class="co">#&gt; # A tibble: 1 x 2</span>
<span class="co">#&gt;   accuracy uncertainty</span>
<span class="co">#&gt;      &lt;dbl&gt;       &lt;dbl&gt;</span>
<span class="co">#&gt; 1    0.209     0.00335</span></pre></body></html></div>
<p>In trying to reduce the size (in physical memory) of your text-predictor, it might be useful to prune the model dictionary. The following command plots an histogram of the distribution of correct predictions in our test.</p>
<div class="sourceCode" id="cb13"><html><body><pre class="r"><span class="kw">if</span>(<span class="fu"><a href="https://rdrr.io/r/base/library.html">require</a></span>(<span class="no">ggplot2</span>)){
        <span class="no">eval</span> <span class="kw">%&gt;%</span>
                <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span>(<span class="no">correct</span>, <span class="no">true</span> <span class="kw">!=</span> <span class="st">"."</span>) <span class="kw">%&gt;%</span>
                <span class="fu">transmute</span>(<span class="kw">rank</span> <span class="kw">=</span> <span class="fu"><a href="https://rdrr.io/r/base/match.html">match</a></span>(<span class="no">true</span>, <span class="kw">table</span> <span class="kw">=</span> <span class="no">sbo</span>$<span class="no">dict</span>)) <span class="kw">%&gt;%</span>
                <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span>(<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span>(<span class="kw">x</span> <span class="kw">=</span> <span class="no">rank</span>)) + <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span>(<span class="kw">binwidth</span> <span class="kw">=</span> <span class="fl">25</span>)
}
<span class="co">#&gt; Carico il pacchetto richiesto: ggplot2</span></pre></body></html></div>
<p><img src="sbo_files/figure-html/unnamed-chunk-13-1.png" width="700"></p>
<p>Apparently, the large majority of correct predictions come from the first ~ 300 words of the dictionary, so that if we prune the dictionary excluding words with rank greater than, <em>e.g.</em>, 500 we can reduce the size of our model without seriously affecting its prediction accuracy.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Valerio Gherardi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
